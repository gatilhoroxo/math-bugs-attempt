# Problemas Avan√ßados de Programa√ß√£o

## üéØ Meta

Integrar √°lgebra linear em projetos pr√°ticos: compress√£o SVD, PCA para redu√ß√£o de dimensionalidade, e solvers eficientes para sistemas grandes.

---

## ‚è±Ô∏è Tempo Estimado

- **Problema 1 (SVD Compress√£o):** 1h-1h30
- **Problema 2 (PCA):** 1h15-1h45
- **Problema 3 (Solver Grandes Sistemas):** 1h30-2h
- **Total:** ~4h-5h15

---

## üìã Pr√©-requisitos

- **Teoria:** `k1-teoria/t3-autovalores.md`, `t4-decomposicoes.md`
- **Implementa√ß√£o:** `k3-implementacao/codigo/i4-decomposicoes.md`
- **Exerc√≠cios:** `k2-exercicios/e3-autovalores-exercicios.md`, `e4-decomposicoes-exercicios.md` (completos)

---

## üéöÔ∏è Dificuldade

‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muito Avan√ßado

---

## üí™ Sistema de XP

- **Problema 1:** 120 XP
- **Problema 2:** 130 XP
- **Problema 3:** 150 XP

**XP Total Dispon√≠vel:** 400 XP

---

## üìä Rastreamento de Progresso

- [ ] Problema 1: Compressor de Imagens SVD (0/1) - 120 XP
- [ ] Problema 2: PCA para Visualiza√ß√£o de Dados (0/1) - 130 XP
- [ ] Problema 3: Solver Iterativo para Sistemas Grandes (0/1) - 150 XP

**XP Conquistado:** ___ / 400 XP

---

## Problema 1: Compressor de Imagens com SVD

### üéØ Objetivo

Implementar compressor de imagens em escala de cinza usando Decomposi√ß√£o em Valores Singulares (SVD), permitindo controlar taxa de compress√£o vs qualidade.

### üìê Contexto Te√≥rico

SVD decomp√µe matriz `A (m√ón)` em:
```
A = U Œ£ V·µÄ
```

Onde:
- `U (m√óm)`: autovetores de `AA·µÄ` (linhas)
- `Œ£ (m√ón)`: valores singulares na diagonal (œÉ‚ÇÅ ‚â• œÉ‚ÇÇ ‚â• ... ‚â• 0)
- `V·µÄ (n√ón)`: autovetores de `A·µÄA` (colunas)

**Compress√£o:** Manter apenas k maiores valores singulares:
```
A_aprox = U_k Œ£_k V_k·µÄ
```

Erro: `||A - A_aprox||_F = ‚àö(œÉ¬≤_(k+1) + œÉ¬≤_(k+2) + ... + œÉ¬≤_r)`

### üõ†Ô∏è Especifica√ß√£o

**Entrada:**
- Imagem grayscale (matriz de pixels 0-255)
- Par√¢metro k (n√∫mero de componentes a manter)

**Sa√≠da:**
- Imagem comprimida (reconstru√≠da com rank k)
- Taxa de compress√£o
- Erro de reconstru√ß√£o (RMSE - Root Mean Square Error)

**Funcionalidades:**
1. Ler imagem PGM (formato simples ASCII)
2. Calcular SVD (pode usar biblioteca ou implementar simplificado)
3. Comprimir mantendo k componentes
4. Reconstruir imagem
5. Calcular m√©tricas (taxa compress√£o, RMSE)
6. Salvar resultado

### üìù Pseudoc√≥digo

```
FUN√á√ÉO comprimir_svd(imagem, k):
    """
    Comprime imagem usando SVD de rank k
    """
    m, n = dimensoes(imagem)
    
    // Calcular SVD: A = UŒ£V·µÄ
    U, sigma, Vt = calcular_svd(imagem)
    
    // Truncar para rank k
    U_k = U[:, 0:k]  // Primeiras k colunas
    sigma_k = sigma[0:k]  // Primeiros k valores
    Vt_k = Vt[0:k, :]  // Primeiras k linhas
    
    // Reconstruir: A_aprox = U_k * diag(sigma_k) * Vt_k
    imagem_comprimida = multiplicar(U_k, multiplicar(diag(sigma_k), Vt_k))
    
    // Calcular m√©tricas
    original_bytes = m * n * 8  // 8 bytes por double
    comprimido_bytes = k * (m + n + 1) * 8  // U_k + sigma_k + Vt_k
    taxa_compressao = original_bytes / comprimido_bytes
    
    rmse = raiz_quadrada(media((imagem - imagem_comprimida)¬≤))
    
    RETORNAR {imagem_comprimida, taxa_compressao, rmse}

FUN√á√ÉO calcular_svd_simplificado(A):
    """
    SVD via autovalores (apenas valores singulares e vetores)
    """
    // Calcular A·µÄA
    AtA = multiplicar(transpor(A), A)
    
    // Autovalores e autovetores de A·µÄA
    lambdas, V = calcular_autovalores_autovetores(AtA)
    
    // Valores singulares: œÉ·µ¢ = ‚àöŒª·µ¢
    sigma = raiz_quadrada(lambdas)
    
    // Calcular U: u·µ¢ = (1/œÉ·µ¢) * A * v·µ¢
    m, n = dimensoes(A)
    U = criar_matriz_zero(m, n)
    
    PARA i DE 0 AT√â n-1:
        SE sigma[i] > EPSILON:
            v_i = V[:, i]
            Av = multiplicar_matriz_vetor(A, v_i)
            U[:, i] = escalar_multiplicar(Av, 1.0 / sigma[i])
    
    Vt = transpor(V)
    
    RETORNAR {U, sigma, Vt}
```

### üß™ Testes

```
Imagem de teste: Matriz 8√ó8 com gradiente simples

A = [0  32  64  96  128 160 192 224]
    [0  32  64  96  128 160 192 224]
    ...

Comprimir com k=2 (de rank 8)

Esperado:
- Taxa compress√£o > 2x
- RMSE baixo (imagem simples)
- Valores singulares decaem rapidamente
```

### üí° Insights

- Imagens naturais t√™m energia concentrada nos primeiros componentes (baixa frequ√™ncia)
- k=10-50 geralmente suficiente para 512√ó512
- Trade-off compress√£o vs qualidade controlado por k

### üéØ Desafios Extras (+30 XP cada)

1. **Compress√£o por blocos:** Dividir imagem em blocos 8√ó8 (como JPEG)
2. **Gr√°fico de energia:** Plotar energia cumulativa vs k
3. **Limiar autom√°tico:** Escolher k para manter 95% da energia

---

## Problema 2: PCA para Visualiza√ß√£o de Dados

### üéØ Objetivo

Implementar Principal Component Analysis (PCA) para reduzir dataset de alta dimensionalidade para 2D/3D, permitindo visualiza√ß√£o.

### üìê Contexto Te√≥rico

PCA encontra dire√ß√µes de m√°xima vari√¢ncia nos dados:

1. **Centralizar:** `X_centered = X - mean(X)`
2. **Covari√¢ncia:** `C = (1/n) * X_centered·µÄ * X_centered`
3. **Autovalores:** Decomposi√ß√£o de C
4. **Projetar:** `X_pca = X_centered * V_k` (k primeiros autovetores)

**Vari√¢ncia explicada:** `Œª·µ¢ / Œ£Œª‚±º`

### üõ†Ô∏è Especifica√ß√£o

**Entrada:**
- Dataset num√©rico (matriz n√ód: n amostras, d features)
- N√∫mero de componentes k (geralmente 2 ou 3)
- Labels das amostras (opcional, para colorir gr√°fico)

**Sa√≠da:**
- Dados projetados em k dimens√µes
- Vari√¢ncia explicada por cada componente
- Componentes principais (dire√ß√µes)

**Funcionalidades:**
1. Carregar dataset (CSV ou formato simples)
2. Centralizar dados
3. Calcular matriz de covari√¢ncia
4. Encontrar autovalores/autovetores
5. Projetar em k dimens√µes
6. Salvar resultado (pontos 2D/3D)

### üìù Pseudoc√≥digo

```
FUN√á√ÉO pca(dataset, k):
    """
    Reduz dataset de d dimens√µes para k dimens√µes
    
    dataset: matriz n√ód (n amostras, d features)
    k: n√∫mero de componentes principais
    """
    n, d = dimensoes(dataset)
    
    // Passo 1: Centralizar dados (m√©dia = 0)
    medias = calcular_media_colunas(dataset)
    dataset_centrado = criar_matriz_zero(n, d)
    
    PARA i DE 0 AT√â n-1:
        PARA j DE 0 AT√â d-1:
            dataset_centrado[i][j] = dataset[i][j] - medias[j]
    
    // Passo 2: Matriz de covari√¢ncia C = (1/n) * X^T * X
    Xt = transpor(dataset_centrado)
    C = multiplicar(Xt, dataset_centrado)
    
    PARA i DE 0 AT√â d-1:
        PARA j DE 0 AT√â d-1:
            C[i][j] /= n
    
    // Passo 3: Autovalores e autovetores de C
    autovalores, autovetores = calcular_eigen(C)
    
    // Passo 4: Ordenar por autovalor decrescente
    indices_ordenados = ordenar_decrescente(autovalores)
    
    autovalores_sorted = reordenar(autovalores, indices_ordenados)
    autovetores_sorted = reordenar_colunas(autovetores, indices_ordenados)
    
    // Passo 5: Selecionar k primeiros autovetores
    V_k = autovetores_sorted[:, 0:k]
    
    // Passo 6: Projetar dados
    dataset_pca = multiplicar(dataset_centrado, V_k)
    
    // Calcular vari√¢ncia explicada
    variancia_total = somar(autovalores_sorted)
    variancia_explicada = criar_array(k)
    
    PARA i DE 0 AT√â k-1:
        variancia_explicada[i] = autovalores_sorted[i] / variancia_total
    
    RETORNAR {
        dataset_pca,
        variancia_explicada,
        componentes_principais: V_k,
        medias: medias
    }

FUN√á√ÉO projetar_novo_ponto(ponto, medias, componentes_principais):
    """Projeta novo ponto usando PCA j√° calculado"""
    ponto_centrado = subtrair(ponto, medias)
    ponto_pca = multiplicar_vetor(transpor(componentes_principais), ponto_centrado)
    RETORNAR ponto_pca
```

### üß™ Testes

```
Dataset de teste: Iris (4 features ‚Üí 2D)

Dados:
- 150 amostras
- 4 features (sepal length, sepal width, petal length, petal width)
- 3 classes (setosa, versicolor, virginica)

Esperado:
- PC1 explica ~73% da vari√¢ncia
- PC2 explica ~23%
- Total 2 componentes: ~96%
- Classes separ√°veis no espa√ßo 2D
```

### üí° Insights

- PCA assume dados Gaussianos e rela√ß√µes lineares
- Normalizar features antes (StandardScaler) se escalas diferentes
- Whitening: `X_white = X_pca / ‚àöŒª` (mesma vari√¢ncia em todas dire√ß√µes)

### üéØ Desafios Extras (+30 XP cada)

1. **Scree plot:** Gr√°fico autovalores vs componente (cotovelo indica k ideal)
2. **Biplot:** Visualizar dados E vetores de features simultaneamente
3. **Kernel PCA:** Vers√£o n√£o-linear usando kernel trick

---

## Problema 3: Solver Iterativo para Sistemas Grandes

### üéØ Objetivo

Implementar m√©todo iterativo de Gauss-Seidel para resolver sistemas lineares grandes e esparsos `Ax = b` de forma eficiente.

### üìê Contexto Te√≥rico

**M√©todos diretos (LU, Cholesky):** O(n¬≥), invi√°vel para n > 10‚Å¥

**M√©todos iterativos:**
- Come√ßar com chute inicial x‚Å∞
- Refinar iterativamente: x^(k+1) = f(x^k)
- Convergir para solu√ß√£o exata

**Gauss-Seidel:**
```
Para i = 1 at√© n:
    x_i^(k+1) = (b_i - Œ£(j<i) a_ij*x_j^(k+1) - Œ£(j>i) a_ij*x_j^(k)) / a_ii
```

Usa valores **j√° atualizados** (mais r√°pido que Jacobi).

**Converg√™ncia:** Garantida se A √© estritamente diagonalmente dominante:
```
|a_ii| > Œ£(j‚â†i) |a_ij|  para todo i
```

### üõ†Ô∏è Especifica√ß√£o

**Entrada:**
- Matriz A (pode ser esparsa, formato COO/CSR)
- Vetor b
- Chute inicial x‚Å∞ (ou vetor zero)
- Toler√¢ncia (ex: 1e-6)
- M√°ximo de itera√ß√µes

**Sa√≠da:**
- Solu√ß√£o aproximada x
- N√∫mero de itera√ß√µes
- Res√≠duo final: ||Ax - b||

**Funcionalidades:**
1. Verificar diagonal domin√¢ncia
2. Iterar Gauss-Seidel at√© converg√™ncia
3. Calcular res√≠duo a cada itera√ß√£o
4. Retornar hist√≥rico de converg√™ncia

### üìù Pseudoc√≥digo

```
FUN√á√ÉO gauss_seidel(A, b, x_inicial, tolerancia, max_iter):
    """
    Resolve Ax = b iterativamente
    
    Converg√™ncia: ||x^(k+1) - x^k|| < tolerancia
    """
    n = tamanho(b)
    x = copiar(x_inicial)
    x_anterior = copiar(x)
    historico_residuo = criar_array(max_iter)
    
    PARA k DE 0 AT√â max_iter-1:
        // Atualizar cada componente
        PARA i DE 0 AT√â n-1:
            soma = 0
            
            // Soma com valores J√Å ATUALIZADOS (j < i)
            PARA j DE 0 AT√â i-1:
                soma += A[i][j] * x[j]
            
            // Soma com valores ANTIGOS (j > i)
            PARA j DE i+1 AT√â n-1:
                soma += A[i][j] * x_anterior[j]
            
            x[i] = (b[i] - soma) / A[i][i]
        
        // Calcular res√≠duo: r = Ax - b
        residuo = calcular_residuo(A, x, b, n)
        historico_residuo[k] = norma(residuo)
        
        // Verificar converg√™ncia
        erro = norma_diferenca(x, x_anterior)
        
        SE erro < tolerancia:
            RETORNAR {
                solucao: x,
                iteracoes: k+1,
                residuo_final: historico_residuo[k],
                historico: historico_residuo[0:k+1]
            }
        
        // Preparar pr√≥xima itera√ß√£o
        x_anterior = copiar(x)
    
    RETORNAR {
        solucao: x,
        iteracoes: max_iter,
        residuo_final: historico_residuo[max_iter-1],
        historico: historico_residuo,
        convergiu: FALSO
    }

FUN√á√ÉO verificar_diagonal_dominante(A, n):
    """
    Verifica se |a_ii| > Œ£|a_ij| (j‚â†i)
    """
    PARA i DE 0 AT√â n-1:
        soma_fora_diagonal = 0
        
        PARA j DE 0 AT√â n-1:
            SE j != i:
                soma_fora_diagonal += abs(A[i][j])
        
        SE abs(A[i][i]) <= soma_fora_diagonal:
            RETORNAR FALSO
    
    RETORNAR VERDADEIRO

FUN√á√ÉO calcular_residuo(A, x, b, n):
    """Res√≠duo: r = Ax - b"""
    Ax = multiplicar_matriz_vetor(A, x)
    residuo = criar_vetor(n)
    
    PARA i DE 0 AT√â n-1:
        residuo[i] = Ax[i] - b[i]
    
    RETORNAR residuo
```

### üß™ Testes

```
Sistema diagonal dominante:
A = [4  1  0]    b = [5]
    [1  4  1]        [6]
    [0  1  4]        [5]

Solu√ß√£o exata: x = [1, 1, 1]

Esperado:
- Converg√™ncia em ~10-15 itera√ß√µes (tol=1e-6)
- Res√≠duo final < 1e-6
- Verifica√ß√£o diagonal dominante: TRUE

Sistema N√ÉO diagonal dominante:
A = [1  2  0]
    [2  1  1]
    [0  1  1]

Esperado:
- Pode n√£o convergir ou convergir lentamente
- Aviso: "Matriz n√£o √© diagonalmente dominante"
```

### üí° Insights

- **SOR (Successive Over-Relaxation):** Acelera Gauss-Seidel com par√¢metro œâ
- **Pr√©-condicionamento:** Transformar sistema para melhor converg√™ncia
- **Matrizes esparsas:** Guardar apenas elementos n√£o-zero (economia massiva)

### üéØ Desafios Extras (+40 XP cada)

1. **Suporte a matriz esparsa:** Formato CSR (Compressed Sparse Row)
2. **SOR com œâ √≥timo:** Encontrar par√¢metro de relaxa√ß√£o ideal
3. **Gradiente Conjugado:** M√©todo mais avan√ßado para matrizes sim√©tricas positivas definidas
4. **Compara√ß√£o de performance:** Gauss-Seidel vs LU para n = 100, 1000, 10000

---

## üéì Entreg√°veis

Para cada problema:
1. ‚úÖ C√≥digo-fonte completo (C/C++/Python)
2. ‚úÖ Arquivo README com instru√ß√µes de compila√ß√£o/execu√ß√£o
3. ‚úÖ Testes com dados reais ou sint√©ticos
4. ‚úÖ Relat√≥rio breve (1-2 p√°ginas):
   - Resultados obtidos
   - Gr√°ficos (se aplic√°vel)
   - An√°lise de performance
   - Desafios encontrados

---

## üéØ Pr√≥ximos Passos

Ap√≥s completar estes problemas:
1. ‚úÖ **Projeto final:** `k5-projeto/` - Motor de transforma√ß√µes 2D
2. ‚úÖ **Aplica√ß√µes avan√ßadas:** Deep learning (backprop usa muito AL), f√≠sica computacional
3. üéâ **Parab√©ns:** Voc√™ dominou √Ålgebra Linear aplicada!

---

**Total XP dispon√≠vel:** 400 XP (+ 210 XP extras)  
**Tempo total estimado:** 4h-5h15 (+ extras)  
**Dificuldade:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Muito Avan√ßado)
