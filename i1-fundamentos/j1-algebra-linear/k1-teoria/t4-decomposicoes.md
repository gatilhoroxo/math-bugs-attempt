# DecomposiÃ§Ãµes Matriciais

## ğŸ¯ Meta de Aprendizado

Ao completar este tÃ³pico, vocÃª serÃ¡ capaz de:
- Entender decomposiÃ§Ãµes como "fatoraÃ§Ãµes" de matrizes
- Aplicar LU, QR, Cholesky para resolver sistemas eficientemente
- Usar SVD para compressÃ£o, recomendaÃ§Ã£o e anÃ¡lise de dados
- Escolher decomposiÃ§Ã£o apropriada para cada problema

---

## â±ï¸ Tempo Estimado

- **Leitura ativa:** 60-75 min
- **ExercÃ­cios relacionados:** 50-65 min (`k2-exercicios/e4-decomposicoes-exercicios.md`)
- **ImplementaÃ§Ã£o:** 80-110 min (`k3-implementacao/codigo/i4-decomposicoes.md`)
- **Total:** ~3h10-4h10

---

## ğŸ“‹ PrÃ©-requisitos

- [x] **t1-vetores-espacos.md** (bases ortonormais)
- [x] **t2-transformacoes-lineares.md** (sistemas lineares, inversas)
- [x] **t3-autovalores-autovetores.md** (ESSENCIAL para SVD)
- [ ] Conforto com matrizes grandes

---

## ğŸ—ºï¸ Mapa Mental

```
DECOMPOSIÃ‡Ã•ES MATRICIAIS
â”œâ”€â”€ 1. Por que Decompor?
â”‚   â”œâ”€â”€ Revelar estrutura oculta
â”‚   â”œâ”€â”€ Simplificar computaÃ§Ãµes
â”‚   â”œâ”€â”€ Estabilidade numÃ©rica
â”‚   â””â”€â”€ CompressÃ£o de dados
â”‚
â”œâ”€â”€ 2. LU (Lower-Upper)
â”‚   â”œâ”€â”€ A = LU
â”‚   â”œâ”€â”€ Resolver sistemas mÃºltiplos (mesmo A, vÃ¡rios b)
â”‚   â”œâ”€â”€ Complexidade O(nÂ³)
â”‚   â””â”€â”€ Pivotamento para estabilidade
â”‚
â”œâ”€â”€ 3. QR (Ortogonal-Triangular)
â”‚   â”œâ”€â”€ A = QR
â”‚   â”œâ”€â”€ Q: ortogonal, R: triangular superior
â”‚   â”œâ”€â”€ Gram-Schmidt
â”‚   â”œâ”€â”€ Resolver sistemas (mais estÃ¡vel que LU)
â”‚   â””â”€â”€ Algoritmo QR para autovalores
â”‚
â”œâ”€â”€ 4. Cholesky
â”‚   â”œâ”€â”€ A = LLáµ€ (simÃ©trica positiva definida)
â”‚   â”œâ”€â”€ Metade do custo de LU
â”‚   â””â”€â”€ ML: inverter matrizes de covariÃ¢ncia
â”‚
â””â”€â”€ 5. SVD (Singular Value Decomposition)
    â”œâ”€â”€ A = UÎ£Váµ€ (QUALQUER matriz!)
    â”œâ”€â”€ U, V: ortogonais
    â”œâ”€â”€ Î£: diagonal (valores singulares)
    â”œâ”€â”€ AplicaÃ§Ãµes incrÃ­veis:
    â”‚   â”œâ”€â”€ CompressÃ£o de imagens
    â”‚   â”œâ”€â”€ RecomendaÃ§Ã£o (Netflix)
    â”‚   â”œâ”€â”€ PCA
    â”‚   â””â”€â”€ Pseudo-inversa
    â””â”€â”€ Algoritmo mais caro, mas mais versÃ¡til
```

---

## ğŸ“– ConteÃºdo

### 1. Por que DecomposiÃ§Ãµes?

Assim como fatoramos `12 = 2Â² Ã— 3`, podemos "fatorar" matrizes!

**Vantagens:**

1. **Revelar estrutura:** Separar informaÃ§Ã£o importante de ruÃ­do
2. **Simplificar cÃ¡lculos:** Resolver sistemas, inverter matrizes
3. **Estabilidade numÃ©rica:** MÃ©todos mais robustos a erros de arredondamento
4. **CompressÃ£o:** Representar dados grandes com menos informaÃ§Ã£o
5. **AplicaÃ§Ãµes especÃ­ficas:** Cada decomposiÃ§Ã£o tem seu "superpoder"

> ğŸ’¡ **IntuiÃ§Ã£o:** Decompor = quebrar problema complexo em pedaÃ§os simples e bem comportados.

**Exemplo conceitual:**
```
Resolver Ax = b diretamente: difÃ­cil
Decompor A = LU, resolver Ly = b e Ux = y: fÃ¡cil (triangulares!)
```

---

### 2. DecomposiÃ§Ã£o LU

#### Conceito

Fatorar matriz `A` em:

```
A = L * U
```

- `L`: **Lower** triangular (triangular inferior)
- `U`: **Upper** triangular (triangular superior)

**Exemplo:**
```
A = [2  1  1]
    [4  3  3]
    [8  7  9]

L = [1  0  0]      U = [2  1  1]
    [2  1  0]          [0  1  1]
    [4  3  1]          [0  0  2]

VerificaÃ§Ã£o: L*U = A âœ“
```

#### Para que Serve?

**Resolver sistemas `Ax = b` para mÃºltiplos `b`:**

```
Passo 1: Fatorar A = LU (uma vez, O(nÂ³))
Passo 2: Para cada b:
  2a. Resolver Ly = b    (substituiÃ§Ã£o direta, O(nÂ²))
  2b. Resolver Ux = y    (substituiÃ§Ã£o reversa, O(nÂ²))
```

**Quando usar:** Precisa resolver `Ax = b` para muitos valores diferentes de `b`.

**Exemplo numÃ©rico:**
```
A = [2  1]      b = [3]
    [4  3]          [7]

L = [1  0]      U = [2  1]
    [2  1]          [0  1]

Ly = b:
[1 0] [yâ‚] = [3]  â†’  yâ‚ = 3
[2 1] [yâ‚‚]   [7]      2*3 + yâ‚‚ = 7  â†’  yâ‚‚ = 1

Ux = y:
[2 1] [xâ‚] = [3]  â†’  2xâ‚ + xâ‚‚ = 3
[0 1] [xâ‚‚]   [1]      xâ‚‚ = 1  â†’  xâ‚ = 1

SoluÃ§Ã£o: x = (1, 1)
```

#### Pivotamento

**Problema:** DivisÃ£o por zero ou nÃºmeros muito pequenos (instabilidade numÃ©rica)

**SoluÃ§Ã£o:** **Pivotamento parcial** - trocar linhas para colocar maior elemento no pivÃ´

```
A = PLU
```

- `P`: matriz de permutaÃ§Ã£o (troca de linhas)

> âš ï¸ **Armadilha:** LU sem pivotamento pode falhar mesmo quando A Ã© invertÃ­vel!

#### Complexidade

- FatoraÃ§Ã£o: **O(nÂ³)**
- Cada soluÃ§Ã£o: **O(nÂ²)**

---

### 3. DecomposiÃ§Ã£o QR

#### Conceito

Fatorar matriz `A` em:

```
A = Q * R
```

- `Q`: matriz **ortogonal** (Qáµ€Q = I)
- `R`: matriz triangular **superior** (Right/Upper)

**Propriedades de Q:**
- Colunas sÃ£o ortonormais
- `Qâ»Â¹ = Qáµ€` (super barato inverter!)
- Preserva normas: `||Qx|| = ||x||`

#### Gram-Schmidt (mÃ©todo para construir QR)

**Ideia:** Ortonormalizar colunas de A

```
Dado: aâ‚, aâ‚‚, aâ‚ƒ (colunas de A)

1. uâ‚ = aâ‚
   qâ‚ = uâ‚ / ||uâ‚||

2. uâ‚‚ = aâ‚‚ - (aâ‚‚Â·qâ‚)qâ‚       (remove componente paralela a qâ‚)
   qâ‚‚ = uâ‚‚ / ||uâ‚‚||

3. uâ‚ƒ = aâ‚ƒ - (aâ‚ƒÂ·qâ‚)qâ‚ - (aâ‚ƒÂ·qâ‚‚)qâ‚‚
   qâ‚ƒ = uâ‚ƒ / ||uâ‚ƒ||

Q = [qâ‚ | qâ‚‚ | qâ‚ƒ]
R = Qáµ€A  (triangular superior por construÃ§Ã£o)
```

**Exemplo:**
```
A = [1  1]
    [0  1]
    [0  0]

Gram-Schmidt:
aâ‚ = (1,0,0)  â†’  qâ‚ = (1,0,0)

aâ‚‚ = (1,1,0)
uâ‚‚ = aâ‚‚ - (aâ‚‚Â·qâ‚)qâ‚ = (1,1,0) - 1*(1,0,0) = (0,1,0)
qâ‚‚ = (0,1,0)

Q = [1  0]      R = [1  1]
    [0  1]          [0  1]
    [0  0]          [0  0]
```

> ğŸ”— **ConexÃ£o:** Gram-Schmidt Ã© usado para criar bases ortonormais (vimos em t1)!

#### Para que Serve?

**1. Resolver sistemas (mais estÃ¡vel que LU):**
```
Ax = b
QRx = b
Rx = Qáµ€b  (multiplicaÃ§Ã£o barata!)
```

**2. MÃ­nimos quadrados:**
```
Minimizar ||Ax - b||Â²
SoluÃ§Ã£o: x = Râ»Â¹Qáµ€b
```

**3. Calcular autovalores (Algoritmo QR iterativo):**
```
Repetir:
  A = QR
  A â† RQ
Converge para matriz triangular (autovalores na diagonal)
```

#### Complexidade

- FatoraÃ§Ã£o: **O(mnÂ²)** (m â‰¥ n)
- Mais caro que LU, mas mais estÃ¡vel numericamente

---

### 4. DecomposiÃ§Ã£o de Cholesky

#### Conceito

Para matrizes **simÃ©tricas positivas definidas** (ex: matrizes de covariÃ¢ncia):

```
A = L * Láµ€
```

- `L`: triangular inferior

**CondiÃ§Ã£o:** A simÃ©trica e todos autovalores `> 0`

**Vantagem:** Metade do custo de LU!

**Exemplo:**
```
A = [4  2]
    [2  3]

L = [2    0  ]
    [1  âˆš2  ]

VerificaÃ§Ã£o: L*Láµ€ = [2  0] [2  1  ] = [4  2] âœ“
                     [1  âˆš2] [0  âˆš2]   [2  3]
```

#### Para que Serve?

**Machine Learning:**
- Inverter matrizes de covariÃ¢ncia (regressÃ£o linear)
- Amostrar distribuiÃ§Ãµes gaussianas multivariadas

**Exemplo (geraÃ§Ã£o de dados correlacionados):**
```python
# Gerar amostras N(0, Î£)
L = cholesky(Î£)
z = random.normal(0, 1, n)  # N(0, I)
x = L @ z  # N(0, Î£)
```

> ğŸ”— **ConexÃ£o:** Em ML, quase toda matriz de covariÃ¢ncia pode usar Cholesky!

#### Complexidade

- FatoraÃ§Ã£o: **O(nÂ³/3)** (metade de LU)

> âš ï¸ **Armadilha:** Se A nÃ£o for positiva definida, Cholesky falha! Teste autovalores antes.

---

### 5. SVD (Singular Value Decomposition)

#### Conceito

**A decomposiÃ§Ã£o mais poderosa!** Funciona para **QUALQUER** matriz (nÃ£o precisa ser quadrada):

```
A = U * Î£ * Váµ€
```

- `U` (mÃ—m): matriz ortogonal (autovetores de AAáµ€)
- `Î£` (mÃ—n): diagonal (valores singulares Ïƒâ‚ â‰¥ Ïƒâ‚‚ â‰¥ ... â‰¥ 0)
- `V` (nÃ—n): matriz ortogonal (autovetores de Aáµ€A)

**Forma reduzida (rank r):**
```
A â‰ˆ U_r * Î£_r * V_ráµ€
```

MantÃ©m r maiores valores singulares, descarta resto (compressÃ£o!)

> ğŸ’¡ **IntuiÃ§Ã£o:** SVD = "receita universal" para decompor QUALQUER transformaÃ§Ã£o linear em rotaÃ§Ãµes e escalas.

#### InterpretaÃ§Ã£o GeomÃ©trica

SVD diz: "Toda matriz faz 3 coisas":

1. **Váµ€**: RotaÃ§Ã£o em â„â¿
2. **Î£**: Escala eixos (Ïƒâ‚, Ïƒâ‚‚, ...)
3. **U**: RotaÃ§Ã£o em â„áµ

**Exemplo:**
```
Matriz 2Ã—2 qualquer = RotaÃ§Ã£oâ‚ â†’ Escala â†’ RotaÃ§Ã£oâ‚‚
```

#### RelaÃ§Ã£o com Autovalores

**Para matriz quadrada simÃ©trica `A = Aáµ€`:**
- Valores singulares = |autovalores|
- U = V = autovetores

**Caso geral:**
- Ïƒáµ¢Â² sÃ£o autovalores de Aáµ€A (ou AAáµ€)
- SVD generaliza diagonalizaÃ§Ã£o para matrizes nÃ£o-quadradas!

#### Como Calcular

**Passo 1:** Calcular Aáµ€A (nÃ—n)

**Passo 2:** Encontrar autovalores/autovetores de Aáµ€A
```
Aáµ€A v_i = Ïƒáµ¢Â² v_i
V = [vâ‚ | vâ‚‚ | ... | vâ‚™]
```

**Passo 3:** Valores singulares
```
Ïƒáµ¢ = âˆš(autovalores de Aáµ€A)
```

**Passo 4:** Calcular U
```
u_i = (1/Ïƒáµ¢) * A * v_i
U = [uâ‚ | uâ‚‚ | ... | uâ‚˜]
```

**Exemplo:**
```
A = [3  0]
    [0  2]
    [0  0]

Aáµ€A = [3 0] [3  0] = [9  0]
      [0 2] [0  2]   [0  4]
      [0 0] [0  0]

Autovalores: Î»â‚ = 9, Î»â‚‚ = 4
Valores singulares: Ïƒâ‚ = 3, Ïƒâ‚‚ = 2

V = [1  0]      Î£ = [3  0]
    [0  1]          [0  2]
                    [0  0]

uâ‚ = (1/3)*A*vâ‚ = (1,0,0)áµ€
uâ‚‚ = (1/2)*A*vâ‚‚ = (0,1,0)áµ€
uâ‚ƒ = (0,0,1)áµ€ (ortogonal aos outros)

U = [1  0  0]
    [0  1  0]
    [0  0  1]
```

---

### 6. AplicaÃ§Ãµes de SVD

#### 1. CompressÃ£o de Imagens

**Ideia:** Manter apenas k maiores valores singulares

```
Imagem original: mÃ—n (ex: 1000Ã—1000 = 1M pixels)

SVD: A = UÎ£Váµ€
AproximaÃ§Ã£o: A â‰ˆ U_k Î£_k V_káµ€

Armazenamento:
  Original: mn
  SVD-k: k(m + n + 1)
  
Para k=50, m=n=1000:
  Original: 1.000.000
  SVD-50: 50(1000 + 1000 + 1) â‰ˆ 100.050  (10Ã— menor!)
```

**Exemplo (imagem em tons de cinza):**
```python
U, S, Vt = np.linalg.svd(img)
k = 50
img_compressed = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
```

#### 2. Sistemas de RecomendaÃ§Ã£o

**Problema:** Matriz usuÃ¡rios Ã— filmes (esparsa, muitos valores faltando)

```
UsuÃ¡rios \ Filmes   | A | B | C | D
--------------------|---|---|---|---
Alice               | 5 | ? | 3 | ?
Bob                 | 4 | 2 | ? | 5
Carol               | ? | 5 | 4 | ?
```

**SoluÃ§Ã£o (SVD):**
```
M â‰ˆ U_k Î£_k V_káµ€

U_k: usuÃ¡rios em "espaÃ§o de preferÃªncias" (k dimensÃµes)
V_k: filmes em "espaÃ§o de gÃªneros" (k dimensÃµes)

Preenche valores faltantes com aproximaÃ§Ã£o!
```

> ğŸ”— **ConexÃ£o:** Netflix Prize (2009) usou variantes de SVD!

#### 3. PCA (Principal Component Analysis)

**PCA usando SVD:**
```python
# Centralizar dados
X_centered = X - X.mean(axis=0)

# SVD
U, S, Vt = np.linalg.svd(X_centered)

# Componentes principais = colunas de V
PC = Vt.T[:, :k]

# Projetar
X_pca = X_centered @ PC
```

**Vantagem sobre autovalores:** SVD Ã© mais estÃ¡vel numericamente que calcular autovalores de Xáµ€X.

#### 4. Pseudo-Inversa (Aâº)

**Para matrizes nÃ£o-inversÃ­veis ou nÃ£o-quadradas:**

```
Aâº = V * Î£âº * Uáµ€

Î£âº: inverter valores singulares nÃ£o-zero
    [Ïƒâ‚  0   0 ]âº   [1/Ïƒâ‚   0    0]
    [ 0  Ïƒâ‚‚  0 ]  = [  0   1/Ïƒâ‚‚  0]
    [ 0   0  0 ]    [  0     0   0]
```

**AplicaÃ§Ã£o:** Resolver sistemas sobre/subdeterminados (mÃ­nimos quadrados)

```
Minimizar ||Ax - b||Â²
SoluÃ§Ã£o: x = Aâºb
```

#### 5. AnÃ¡lise de RuÃ­do

```
Valores singulares grandes: InformaÃ§Ã£o
Valores singulares pequenos: RuÃ­do

Filtrar ruÃ­do: manter apenas k maiores Ïƒáµ¢
```

---

### 7. ComparaÃ§Ã£o de DecomposiÃ§Ãµes

| DecomposiÃ§Ã£o | CondiÃ§Ã£o | Custo | Quando Usar |
|--------------|----------|-------|-------------|
| **LU** | Quadrada (invertÃ­vel) | O(nÂ³) | MÃºltiplos sistemas (mesmo A) |
| **Cholesky** | SimÃ©trica positiva def. | O(nÂ³/3) | CovariÃ¢ncias, ML |
| **QR** | Qualquer | O(mnÂ²) | MÃ­nimos quadrados, estabilidade |
| **SVD** | **Qualquer!** | O(mnÂ²) | CompressÃ£o, recomendaÃ§Ã£o, PCA, pseudo-inversa |

> ğŸ’¡ **Regra geral:** Use decomposiÃ§Ã£o mais especÃ­fica possÃ­vel (mais eficiente). SVD Ã© "canivete suÃ­Ã§o" (funciona sempre, mas mais caro).

---

### 8. Estabilidade NumÃ©rica

**Ranking de estabilidade (melhor â†’ pior):**

1. **SVD** (mais estÃ¡vel, mas mais caro)
2. **QR**
3. **Cholesky** (para simÃ©tricas positivas)
4. **LU com pivotamento**
5. **LU sem pivotamento** (pode falhar!)
6. **Inversa direta** (NUNCA USE!)

> âš ï¸ **Armadilha:** Nunca calcule `x = inv(A) @ b`! Sempre use `x = solve(A, b)` (usa LU internamente).

---

## âœ… Checklist de CompreensÃ£o

VocÃª domina decomposiÃ§Ãµes se consegue:

- [ ] Explicar por que decomposiÃ§Ãµes sÃ£o Ãºteis (3 razÃµes)
- [ ] Usar LU para resolver sistemas mÃºltiplos
- [ ] Aplicar Gram-Schmidt para ortonormalizar vetores
- [ ] Saber quando usar Cholesky vs LU
- [ ] Entender SVD geometricamente (rotaÃ§Ã£o-escala-rotaÃ§Ã£o)
- [ ] Usar SVD para compressÃ£o de dados
- [ ] Explicar diferenÃ§a entre valores singulares e autovalores
- [ ] Escolher decomposiÃ§Ã£o apropriada para problema dado
- [ ] Saber que inversa direta Ã© mÃ¡ ideia numericamente

---

## ğŸ¯ PrÃ³ximos Passos

### 1. **Praticar Agora** (RECOMENDADO)
ğŸ“ FaÃ§a exercÃ­cios: `k2-exercicios/e4-decomposicoes-exercicios.md`
- NÃ­vel 1: LU, QR, Cholesky manualmente (2Ã—2, 3Ã—3)
- NÃ­vel 2: SVD, compressÃ£o de imagens
- NÃ­vel 3: AplicaÃ§Ãµes prÃ¡ticas (recomendaÃ§Ã£o, PCA)

### 2. **Implementar**
ğŸ’» Veja cÃ³digo: `k3-implementacao/codigo/i4-decomposicoes.md`
- Implementar Gram-Schmidt
- SVD usando bibliotecas
- CompressÃ£o de imagens
- Sistema de recomendaÃ§Ã£o simples

### 3. **Aplicar**
ğŸ® Problemas prÃ¡ticos: `k4-pratica/p2-intermediarios.md`
- Problema 2.1: Resolver sistemas eficientemente
- Problema 2.2: Compressor de imagens SVD

### 4. **Projeto Final**
ğŸš€ Integrar tudo: `k5-projeto/`
- Usar decomposiÃ§Ãµes para otimizar transformaÃ§Ãµes
- Implementar aproximaÃ§Ãµes de baixo rank

### 5. **Continuar Aprendendo**
- TÃ³picos avanÃ§ados: Eigenfaces, Kernel PCA, NMF
- Ãlgebra Linear NumÃ©rica (Trefethen & Bau)
- AplicaÃ§Ãµes em Deep Learning (tensores, backprop)

---

## ğŸ“š Recursos Adicionais

### VÃ­deos
- 3Blue1Brown - Nonsquare matrices (Essence ch. 15)
- MIT OCW - Strang Lecture 26-27 (SVD)

### Leitura
- Strang - Introduction to Linear Algebra (Cap. 7)
- Trefethen & Bau - Numerical Linear Algebra (THE book)

### Ferramentas
- NumPy: `np.linalg.svd()`, `np.linalg.qr()`, `scipy.linalg.cholesky()`
- LAPACK (por baixo dos panos): ImplementaÃ§Ãµes ultra-otimizadas

### AplicaÃ§Ãµes Reais
- Scikit-learn: `TruncatedSVD`, `PCA`
- Surprise (recomendaÃ§Ã£o): Usa SVD
- OpenCV: CompressÃ£o de imagens

---

## ğŸ’¬ Notas de Aprendizado

<details>
<summary>ğŸ“ DÃºvidas frequentes que tive</summary>

- 

</details>

<details>
<summary>ğŸ’¡ Insights que tive durante o estudo</summary>

- 

</details>

<details>
<summary>ğŸ”— ConexÃµes que fiz com outros tÃ³picos</summary>

- 

</details>

---

**Tempo gasto neste tÃ³pico:** ___ minutos  
**Data de conclusÃ£o:** ___/___/___  
**RevisÃ£o necessÃ¡ria:** [ ] Sim [ ] NÃ£o

---

## ğŸ‰ ParabÃ©ns!

VocÃª completou todos os tÃ³picos de **teoria** de Ãlgebra Linear!

**O que vocÃª domina agora:**
- âœ… Vetores e espaÃ§os vetoriais
- âœ… TransformaÃ§Ãµes lineares e sistemas
- âœ… Autovalores e autovetores
- âœ… DecomposiÃ§Ãµes matriciais

**PrÃ³ximos desafios:**
1. Fazer TODOS os exercÃ­cios (`k2-exercicios/`)
2. Implementar tudo em cÃ³digo (`k3-implementacao/`)
3. Resolver problemas prÃ¡ticos (`k4-pratica/`)
4. Finalizar projeto completo (`k5-projeto/`)

**Continue assim! ğŸš€**
